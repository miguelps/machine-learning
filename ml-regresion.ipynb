{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWr76VevFL6_"
      },
      "source": [
        "<!-- # Regresi√≥n y Selecci√≥n de Modelos\n",
        "\n",
        "## Aplicaciones en Ingenier√≠a Electr√≥nica\n",
        "\n",
        "Este notebook contiene ejemplos pr√°cticos y aplicaciones de regresi√≥n y selecci√≥n de modelos en el contexto de ingenier√≠a electr√≥nica.\n",
        "\n",
        "<a id=\"contenido\"></a>\n",
        "\n",
        "### Temas Cubiertos:\n",
        "\n",
        "[**Ejemplos de Aplicaciones Pr√°cticas**](#x)\n",
        "\n",
        "1. [**Calibraci√≥n de Sensores**](#calibracion-sensores)\n",
        "2. [**Predicci√≥n de Fallas en Circuitos**](#prediccion-fallas)\n",
        "3. [**Optimizaci√≥n de Par√°metros de Fabricaci√≥n**](#optimizacion-parametros)\n",
        "4. [**Selecci√≥n de Modelos y Validaci√≥n Cruzada**](#seleccion-modelos)\n",
        "5. [**Control de Procesos Industriales**](#control-procesos)\n",
        "\n",
        "[**Resumen y Conclusiones**](#resumen) -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ_eJFf_FL7C"
      },
      "source": [
        "<a id=\"contenido\"></a>\n",
        "\n",
        "\n",
        "# Regresi√≥n y Selecci√≥n de Modelos en Machine Learning\n",
        "\n",
        "\n",
        "## Aplicaciones Pr√°cticas\n",
        "\n",
        "1. **[Calibraci√≥n de Sensores de Temperatura](#calibracion-sensores)**\n",
        "2. **[Optimizaci√≥n de Par√°metros de Fabricaci√≥n de Resistencias](#optimizacion-parametros)**\n",
        "3. **[Selecci√≥n de Modelos y Validaci√≥n Cruzada](#seleccion-modelos)**\n",
        "\n",
        "---\n",
        "\n",
        "### Objetivos de Aprendizaje\n",
        "\n",
        "- Aplicar modelos de regresi√≥n lineal a problemas reales de ingenier√≠a\n",
        "- Comparar diferentes algoritmos: Linear, Ridge, Lasso, Random Forest\n",
        "- Implementar t√©cnicas de validaci√≥n cruzada y optimizaci√≥n de hiperpar√°metros\n",
        "- Evaluar modelos usando m√©tricas apropiadas (RMSE, R¬≤, MAE)\n",
        "- Interpretar resultados y seleccionar el mejor modelo\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETPkULRZFL7E"
      },
      "source": [
        "### Importar librer√≠as necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8FY-ipBFL7F"
      },
      "outputs": [],
      "source": [
        "# Importar librer√≠as necesarias\n",
        "import numpy as np               # Operaciones num√©ricas y arrays\n",
        "import pandas as pd              # Manipulaci√≥n de datos tabulares\n",
        "import matplotlib.pyplot as plt  # Visualizaci√≥n de datos\n",
        "import seaborn as sns            # Visualizaci√≥n estad√≠stica avanzada\n",
        "\n",
        "# Modelos de Machine Learning\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
        "# StandardScaler: Estandarizaci√≥n, PolynomialFeatures: Crear t√©rminos polinomiales\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "# train_test_split: Dividir datos, cross_val_score: Validaci√≥n cruzada, GridSearchCV: Optimizaci√≥n\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "# M√©tricas para evaluaci√≥n de modelos\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, classification_report, confusion_matrix\n",
        "# Modelos de ensemble (combinaci√≥n de m√∫ltiples modelos)\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "\n",
        "# Suprimir advertencias para visualizaci√≥n limpia\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurar estilo de gr√°ficos\n",
        "plt.style.use('seaborn-v0_8')   # Estilo visual moderno\n",
        "sns.set_palette(\"husl\")         # Paleta de colores armoniosa\n",
        "\n",
        "print(\"Librer√≠as importadas correctamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yl9ALXPFL7H"
      },
      "source": [
        "<a id=\"calibracion-sensores\"></a>\n",
        "\n",
        "## 1. Calibraci√≥n de Sensores de Temperatura [‚¨Ü](#contenido)\n",
        "\n",
        "### Problema:\n",
        "Un sensor de temperatura tiene desgaste y no linealidad. Necesitamos crear un modelo de calibraci√≥n que convierta la lectura del sensor (voltaje) a temperatura real.\n",
        "\n",
        "### Datos:\n",
        "- Temperatura real (¬∞C): 5, 25, 50, 75, 100\n",
        "- Lectura del sensor (V): 0.1, 1.2, 2.4, 3.6, 4.8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlhV4aj4FL7J"
      },
      "outputs": [],
      "source": [
        "# Crear datos de calibraci√≥n del sensor\n",
        "\n",
        "# Puntos de referencia conocidos (medidos con term√≥metro de precisi√≥n)\n",
        "temperatura_real = np.array([5, 25, 50, 75, 100])     # ¬∞C, temperatura real medida\n",
        "lectura_sensor = np.array([0.1, 1.2, 2.4, 3.6, 4.8])  #  V, voltaje de salida del sensor\n",
        "\n",
        "# Crear DataFrame para organizar los datos de manera tabular\n",
        "df_sensor = pd.DataFrame({\n",
        "    'lectura_sensor': lectura_sensor,\n",
        "    'temperatura_real': temperatura_real,\n",
        "})\n",
        "\n",
        "print(\"Datos del sensor:\")\n",
        "print(df_sensor)\n",
        "\n",
        "# Visualizar la relaci√≥n\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(lectura_sensor, temperatura_real, s=100, alpha=0.7, color='blue')\n",
        "plt.xlabel('Lectura del Sensor (V)')\n",
        "plt.ylabel('Temperatura Real (¬∞C)')\n",
        "plt.title('Calibraci√≥n de Sensor de Temperatura')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NdgHlHaFL7K"
      },
      "outputs": [],
      "source": [
        "# Ajustar modelo de regresi√≥n lineal\n",
        "\n",
        "# Reshape(-1, 1): Convierte el array 1D en matriz columna (necesario para sklearn)\n",
        "X_sensor = lectura_sensor.reshape(-1, 1)\n",
        "y_sensor = temperatura_real\n",
        "\n",
        "# Modelo lineal simple\n",
        "modelo_lineal = LinearRegression()\n",
        "# fit(): Entrena el modelo encontrando los coeficientes que minimizan el error cuadr√°tico\n",
        "modelo_lineal.fit(X_sensor, y_sensor)\n",
        "\n",
        "# Predecir temperaturas\n",
        "# predict(): Aplica la ecuaci√≥n T = intercept + coef * V a los datos de entrada\n",
        "y_pred_lineal = modelo_lineal.predict(X_sensor)\n",
        "\n",
        "# Calcular m√©tricas\n",
        "\n",
        "# MSE: Error cuadr√°tico medio, penaliza errores grandes\n",
        "mse_lineal = mean_squared_error(y_sensor, y_pred_lineal)\n",
        "\n",
        "# RMSE: Ra√≠z del MSE, en las mismas unidades que la variable objetivo\n",
        "rmse_lineal = np.sqrt(mse_lineal)\n",
        "\n",
        "# R¬≤: Coeficiente de determinaci√≥n, mide qu√© tan bien el modelo explica la variabilidad (0-1)\n",
        "r2_lineal = r2_score(y_sensor, y_pred_lineal)\n",
        "\n",
        "print(f\"Modelo Lineal:\")\n",
        "print(f\"Ecuaci√≥n: T = {modelo_lineal.intercept_:.2f} + {modelo_lineal.coef_[0]:.2f} * V\")\n",
        "print(f\"RMSE: {rmse_lineal:.2f} ¬∞C\")\n",
        "print(f\"R¬≤: {r2_lineal:.4f}\")\n",
        "\n",
        "# Visualizar resultados\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Subgr√°fico 1: Ajuste del modelo\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "plt.scatter(lectura_sensor, temperatura_real, s=100, alpha=0.7, color='blue', label='Datos reales')\n",
        "    # s: tama√±o de puntos, alpha: transparencia\n",
        "\n",
        "# L√≠nea de regresi√≥n (predicciones del modelo)\n",
        "plt.plot(lectura_sensor, y_pred_lineal, 'r-', linewidth=2, label='Modelo lineal')\n",
        "\n",
        "plt.xlabel('Lectura del Sensor (V)')\n",
        "plt.ylabel('Temperatura Real (¬∞C)')\n",
        "plt.title('Calibraci√≥n Lineal')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subgr√°fico 2: An√°lisis de residuos\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "# Residuos: Diferencia entre valor real y predicci√≥n (e_i = y_i - ≈∑_i)\n",
        "residuos = temperatura_real - y_pred_lineal\n",
        "plt.scatter(y_pred_lineal, residuos, s=100, alpha=0.7, color='green')\n",
        "\n",
        "# L√≠nea horizontal en y=0 (residuos ideales deben estar alrededor de 0)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "\n",
        "plt.xlabel('Temperatura Predicha (¬∞C)')\n",
        "plt.ylabel('Residuos (¬∞C)')\n",
        "plt.title('An√°lisis de Residuos')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuVRgXCTFL7L"
      },
      "source": [
        "<a id=\"optimizacion-parametros\"></a>\n",
        "\n",
        "## 2. Optimizaci√≥n de Par√°metros de Fabricaci√≥n de Resistencias [‚¨Ü](#contenido)\n",
        "\n",
        "### Problema:\n",
        "Optimizar los par√°metros de fabricaci√≥n para obtener resistencias con valores espec√≠ficos.\n",
        "\n",
        "### Variables:\n",
        "- **Temperatura de cocci√≥n** (¬∞C)\n",
        "- **Tiempo de cocci√≥n** (minutos)\n",
        "- **Espesor del material** (Œºm)\n",
        "- **Concentraci√≥n de dopante** (%)\n",
        "- **Objetivo**: Resistencia (Œ©)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xSh2En9FL7M"
      },
      "outputs": [],
      "source": [
        "# Generar datos de fabricaci√≥n de resistencias\n",
        "\n",
        "np.random.seed(127)\n",
        "n_resistencias = 150\n",
        "\n",
        "# Par√°metros de fabricaci√≥n\n",
        "\n",
        "# uniform(min, max, n): Distribuci√≥n uniforme (todos los valores igualmente probables)\n",
        "temp_coccion = np.random.uniform(800, 1200, n_resistencias)          # ¬∞C, rango de cocci√≥n t√≠pico\n",
        "tiempo_coccion = np.random.uniform(30, 120, n_resistencias)          # min\n",
        "espesor = np.random.uniform(10, 50, n_resistencias)                  # Œºm, espesor de pel√≠cula\n",
        "concentracion_dopante = np.random.uniform(0.1, 5.0, n_resistencias)  # %, dopaje del material\n",
        "\n",
        "# Modelo f√≠sico simplificado para resistencia\n",
        "# Basado en ley de Ohm: R = œÅ * L / A, donde œÅ depende de temperatura y dopante\n",
        "# exp(): Funci√≥n exponencial, modela efecto no lineal de temperatura\n",
        "resistividad_base = 0.1                             # Œ©¬∑Œºm, resistividad base del material\n",
        "factor_temp = np.exp(-(temp_coccion - 1000) / 200)  # Efecto de temperatura (Arrhenius)\n",
        "factor_dopante = 1 / (1 + concentracion_dopante)    # Dopante reduce resistencia\n",
        "factor_espesor = 1 / espesor                        # Menor espesor = mayor resistencia\n",
        "\n",
        "# Calcular resistencia con modelo f√≠sico m√°s ruido gaussiano (simula imperfecciones)\n",
        "resistencia = (resistividad_base * factor_temp * factor_dopante * factor_espesor *\n",
        "               (1 + 0.1 * np.random.normal(0, 1, n_resistencias)))  # 10% de ruido\n",
        "\n",
        "# Crear DataFrame\n",
        "df_resistencias = pd.DataFrame({\n",
        "    'temp_coccion':          temp_coccion,\n",
        "    'tiempo_coccion':        tiempo_coccion,\n",
        "    'espesor':               espesor,\n",
        "    'concentracion_dopante': concentracion_dopante,\n",
        "    'resistencia':           resistencia\n",
        "})\n",
        "\n",
        "print(\"Datos de fabricaci√≥n de resistencias:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nMuestras:\")\n",
        "df_resistencias.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nEstad√≠sticas descriptivas:\")\n",
        "df_resistencias.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ3A9xzkFL7O"
      },
      "outputs": [],
      "source": [
        "# An√°lisis exploratorio de datos de resistencias\n",
        "\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "# Relaciones entre variables independientes y la variable objetivo (resistencia)\n",
        "variables = ['temp_coccion', 'tiempo_coccion', 'espesor', 'concentracion_dopante']\n",
        "for i, var in enumerate(variables):\n",
        "    plt.subplot(2, 2, i+1)  # Grid 2x2 para visualizar todas las variables\n",
        "\n",
        "    # Gr√°fico de dispersi√≥n para identificar relaciones lineales o no lineales\n",
        "    plt.scatter(df_resistencias[var], df_resistencias['resistencia'], alpha=0.6)\n",
        "\n",
        "    plt.xlabel(var)\n",
        "    plt.ylabel('Resistencia (Œ©)')\n",
        "    plt.title(f'Resistencia vs {var}')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Matriz de correlaci√≥n: Identificar multicolinealidad entre variables\n",
        "correlation_matrix = df_resistencias.corr()\n",
        "    # Valores cercanos a 1/-1: Fuerte correlaci√≥n positiva/negativa\n",
        "    # Valores cercanos a 0: No hay correlaci√≥n lineal\n",
        "\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, fmt='.2f')\n",
        "plt.title('Matriz de Correlaci√≥n - Resistencias')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaLAV75oFL7P"
      },
      "outputs": [],
      "source": [
        "# Entrenar y comparar diferentes modelos de regresi√≥n\n",
        "\n",
        "# Separar variables predictoras (X) y variable objetivo (y)\n",
        "X_resistencias = df_resistencias[['temp_coccion', 'tiempo_coccion', 'espesor', 'concentracion_dopante']]\n",
        "y_resistencias = df_resistencias['resistencia']\n",
        "\n",
        "# Dividir datos: 70% entrenamiento, 30% prueba\n",
        "# test_size=0.3: 30% para test, random_state: Reproducibilidad\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_resistencias, y_resistencias,\n",
        "                                                           test_size=0.3, random_state=42)\n",
        "\n",
        "# Estandarizar: Transformar datos a media=0 y desviaci√≥n=1\n",
        "# Cr√≠tico para Ridge, Lasso y otros modelos sensibles a la escala\n",
        "scaler_r = StandardScaler()\n",
        "X_train_r_scaled = scaler_r.fit_transform(X_train_r)    # Ajustar y transformar train\n",
        "X_test_r_scaled = scaler_r.transform(X_test_r)          # Solo transformar test (no ajustar)\n",
        "\n",
        "# Modelos a comparar\n",
        "modelos = {\n",
        "    'Regresi√≥n Lineal': LinearRegression(),             # Regresi√≥n sin regularizaci√≥n\n",
        "    'Ridge (Œ±=0.1)':    Ridge(alpha=0.1),               # Regularizaci√≥n L2 leve (penaliza coef. grandes)\n",
        "    'Ridge (Œ±=1.0)':    Ridge(alpha=1.0),               # Regularizaci√≥n L2 moderada\n",
        "    'Lasso (Œ±=0.01)':   Lasso(alpha=0.01),              # Regularizaci√≥n L1 leve (puede hacer coef. = 0)\n",
        "    'Lasso (Œ±=0.1)':    Lasso(alpha=0.1),               # Regularizaci√≥n L1 moderada\n",
        "    'Random Forest':    RandomForestRegressor(n_estimators=100, random_state=42)  # Ensemble de 100 √°rboles\n",
        "}\n",
        "\n",
        "resultados = {}\n",
        "\n",
        "print(\"Comparaci√≥n de Modelos de Regresi√≥n:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for nombre_modelo, modelo in modelos.items():\n",
        "    # Entrenar modelo seg√∫n su tipo\n",
        "    if nombre_modelo == 'Random Forest':\n",
        "        # Random Forest: No necesita estandarizaci√≥n (basado en √°rboles)\n",
        "        modelo.fit(X_train_r, y_train_r)\n",
        "        y_pred = modelo.predict(X_test_r)\n",
        "    else:\n",
        "        # Modelos lineales: Requieren datos estandarizados para convergencia √≥ptima\n",
        "        modelo.fit(X_train_r_scaled, y_train_r)\n",
        "        y_pred = modelo.predict(X_test_r_scaled)\n",
        "\n",
        "    # Calcular m√©tricas de regresi√≥n\n",
        "    mse = mean_squared_error(y_test_r, y_pred)   # Error cuadr√°tico medio\n",
        "    rmse = np.sqrt(mse)                          # Ra√≠z del MSE (mismas unidades que objetivo)\n",
        "    mae = mean_absolute_error(y_test_r, y_pred)  # Error absoluto medio (robusto a outliers)\n",
        "    r2 = r2_score(y_test_r, y_pred)              # R¬≤ (proporci√≥n de varianza explicada, 0-1)\n",
        "\n",
        "    resultados[nombre_modelo] = {'RMSE': rmse, 'MAE': mae, 'R¬≤': r2}\n",
        "    # print(f\"{nombre_modelo:20} | RMSE: {rmse:.4f} | MAE: {mae:.4f} | R¬≤: {r2: .4f} |\")\n",
        "\n",
        "# Crear DataFrame de resultados\n",
        "df_resultados = pd.DataFrame(resultados).T\n",
        "print(df_resultados.round(4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfSmBe2tFL7Q"
      },
      "outputs": [],
      "source": [
        "# Visualizar comparaci√≥n de modelos\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Gr√°fico de barras para RMSE (menor es mejor)\n",
        "plt.subplot(1, 3, 1)\n",
        "df_resultados['RMSE'].plot(kind='bar', color='skyblue')\n",
        "plt.title('Comparaci√≥n de RMSE')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico de barras para R¬≤ (m√°s cercano a 1 es mejor)\n",
        "plt.subplot(1, 3, 2)\n",
        "df_resultados['R¬≤'].plot(kind='bar', color='lightgreen')\n",
        "plt.title('Comparaci√≥n de R¬≤')\n",
        "plt.ylabel('R¬≤')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico de dispersi√≥n: Predicho vs Real (mejor modelo)\n",
        "# idxmax(): Retorna el √≠ndice del valor m√°ximo (mejor R¬≤)\n",
        "mejor_modelo = df_resultados['R¬≤'].idxmax()\n",
        "# Reentrenar el mejor modelo para generar predicciones\n",
        "if mejor_modelo == 'Random Forest':\n",
        "    modelo_mejor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    modelo_mejor.fit(X_train_r, y_train_r)\n",
        "    y_pred_mejor = modelo_mejor.predict(X_test_r)\n",
        "else:\n",
        "    modelo_mejor = Ridge(alpha=0.1)\n",
        "    modelo_mejor.fit(X_train_r_scaled, y_train_r)\n",
        "    y_pred_mejor = modelo_mejor.predict(X_test_r_scaled)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(y_test_r, y_pred_mejor, alpha=0.6, color='red')\n",
        "# L√≠nea diagonal perfecta (y=x): Si predicci√≥n = real, punto cae en esta l√≠nea\n",
        "plt.plot([y_test_r.min(), y_test_r.max()], [y_test_r.min(), y_test_r.max()], 'k--', lw=2)\n",
        "plt.xlabel('Resistencia Real (Œ©)')\n",
        "plt.ylabel('Resistencia Predicha (Œ©)')\n",
        "plt.title(f'Predicci√≥n vs Real - {mejor_modelo}')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nMejor modelo: {mejor_modelo}\")\n",
        "print(f\"R¬≤ = {df_resultados.loc[mejor_modelo, 'R¬≤']:.4f}\")\n",
        "print(f\"RMSE = {df_resultados.loc[mejor_modelo, 'RMSE']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDnG2kp9FL7R"
      },
      "source": [
        "<a id=\"seleccion-modelos\"></a>\n",
        "\n",
        "## 3. Selecci√≥n de Modelos y Validaci√≥n Cruzada [‚¨Ü](#contenido)\n",
        "\n",
        "### Problema:\n",
        "Implementar t√©cnicas de selecci√≥n de modelos y validaci√≥n cruzada para encontrar el mejor modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmUsIJ8HFL7R"
      },
      "outputs": [],
      "source": [
        "# Validaci√≥n cruzada para selecci√≥n de hiperpar√°metros\n",
        "\n",
        "# Definir rangos de hiperpar√°metros para Ridge\n",
        "# alpha: Par√°metro de regularizaci√≥n (valores m√°s altos = m√°s regularizaci√≥n)\n",
        "param_grid_ridge = {\n",
        "    'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "}\n",
        "\n",
        "# Grid Search con validaci√≥n cruzada\n",
        "# cv=5: Divide datos en 5 partes, entrena con 4 y valida con 1 (repite 5 veces)\n",
        "# scoring: M√©trica a optimizar (negativo porque sklearn maximiza)\n",
        "ridge_cv = GridSearchCV(\n",
        "    Ridge(),                           # Modelo a optimizar\n",
        "    param_grid_ridge,                  # Grade de par√°metros\n",
        "    cv=5,                              # 5-fold cross-validation\n",
        "    scoring='neg_mean_squared_error'   # M√©trica a optimizar\n",
        ")\n",
        "\n",
        "# Prueba todas las combinaciones de hiperpar√°metros y selecciona la mejor\n",
        "ridge_cv.fit(X_train_r_scaled, y_train_r)\n",
        "\n",
        "print(\"Mejores par√°metros para Ridge:\")\n",
        "\n",
        "# best_params_: Diccionario con los hiperpar√°metros √≥ptimos encontrados\n",
        "print(ridge_cv.best_params_)\n",
        "\n",
        "# best_score_: Mejor score promedio obtenido en validaci√≥n cruzada\n",
        "print(f\"Mejor score (negativo MSE): {ridge_cv.best_score_:.4f}\")\n",
        "\n",
        "# Comparar modelos con validaci√≥n cruzada usando mejores hiperpar√°metros\n",
        "modelos_cv = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge (CV)':        Ridge(alpha=ridge_cv.best_params_['alpha']),  # Usar alpha √≥ptimo\n",
        "    'Lasso':             Lasso(alpha=0.01),\n",
        "    'Random Forest':     RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "print(\"\\nValidaci√≥n Cruzada (5-fold):\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "cv_scores = {}\n",
        "for nombre, modelo in modelos_cv.items():\n",
        "    # cross_val_score: Eval√∫a el modelo usando validaci√≥n cruzada k-fold\n",
        "    # Divide datos en k partes, entrena k veces y promedia resultados\n",
        "    if nombre == 'Random Forest':\n",
        "        # Random Forest no requiere estandarizaci√≥n (invariante a escala)\n",
        "        scores = cross_val_score(modelo, X_train_r, y_train_r, cv=5, scoring='neg_mean_squared_error')\n",
        "    else:\n",
        "        # Modelos lineales requieren datos estandarizados\n",
        "        scores = cross_val_score(modelo, X_train_r_scaled, y_train_r, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "    # Almacenar resultados de validaci√≥n cruzada\n",
        "    cv_scores[nombre] = {\n",
        "        'mean_score': scores.mean(),        # Promedio de scores entre los k folds\n",
        "        'std_score': scores.std(),          # Desviaci√≥n est√°ndar (mide variabilidad)\n",
        "        'rmse_cv': np.sqrt(-scores.mean())  # Convertir MSE negativo a RMSE positivo\n",
        "    }\n",
        "\n",
        "    # Mostrar RMSE ¬± desviaci√≥n (menor RMSE y menor desviaci√≥n = mejor modelo)\n",
        "    print(f\"{nombre:20} | RMSE: {np.sqrt(-scores.mean()):.4f} ¬± {np.sqrt(scores.std()):.4f}\")\n",
        "\n",
        "# Visualizar resultados de validaci√≥n cruzada\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "nombres = list(cv_scores.keys())\n",
        "rmse_values = [cv_scores[n]['rmse_cv'] for n in nombres]\n",
        "std_values = [cv_scores[n]['std_score'] for n in nombres]\n",
        "\n",
        "# Gr√°fico de barras con barras de error (yerr) para mostrar variabilidad entre folds\n",
        "# capsize: Tama√±o de las \"tapas\" en las barras de error\n",
        "plt.bar(nombres, rmse_values, yerr=std_values, capsize=5, alpha=0.7, color='lightcoral')\n",
        "plt.title('RMSE con Validaci√≥n Cruzada')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# Scores negativos porque sklearn maximiza, pero MSE se minimiza\n",
        "plt.bar(nombres, [cv_scores[n]['mean_score'] for n in nombres],\n",
        "        yerr=[cv_scores[n]['std_score'] for n in nombres],\n",
        "        capsize=5, alpha=0.7, color='lightblue')\n",
        "plt.title('Score Promedio (Negativo MSE)')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8whFavIYFL7R"
      },
      "source": [
        "---\n",
        "\n",
        "## Resumen: Caso 1 - Calibraci√≥n de Sensores\n",
        "\n",
        "### Problema de Ingenier√≠a Electr√≥nica\n",
        "- **Sensor de temperatura con desgaste** ‚Üí Relaci√≥n voltaje-temperatura distorsionada\n",
        "- **Soluci√≥n**: Modelo de regresi√≥n lineal para calibraci√≥n\n",
        "\n",
        "### Resultados Obtenidos\n",
        "- **Ecuaci√≥n**: `T = -1.26 + 21.18 √ó V`\n",
        "- **RMSE**: 0.60¬∞C (excelente precisi√≥n)\n",
        "- **R¬≤**: 0.9997 (99.97% de varianza explicada)\n",
        "\n",
        "### Aplicaciones Pr√°cticas\n",
        "- Sistemas de control de temperatura\n",
        "- Instrumentaci√≥n industrial\n",
        "- Monitoreo ambiental\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W66nWFPiFL7S"
      },
      "source": [
        "## Resumen: Caso 2 - Fabricaci√≥n de Resistencias\n",
        "\n",
        "### Problema de Manufactura Compleja\n",
        "- **4 Variables de proceso**: Temperatura, tiempo, espesor, dopante\n",
        "- **Objetivo**: Optimizar resistencia del componente\n",
        "\n",
        "### Comparaci√≥n de Modelos\n",
        "\n",
        "| Modelo              | RMSE       | R¬≤         | Caracter√≠sticas        |\n",
        "|---------------------|-----------:|-----------:|------------------------|\n",
        "| Regresi√≥n Lineal    | 0.0005     |     0.7478 | Simple, interpretable  |\n",
        "| Ridge (Œ±=0.1)       | 0.0005     |     0.7479 | Regularizaci√≥n L2      |\n",
        "| Lasso (Œ±=0.01)      | 0.0009     |     -0.0859| Selecci√≥n de variables |\n",
        "| **Random Forest**   | **0.0004** | **0.8202** | **Mejor rendimiento**  |\n",
        "\n",
        "### Lecciones Aprendidas\n",
        "- **Estandarizaci√≥n crucial** para modelos lineales\n",
        "- **Random Forest** maneja relaciones no lineales\n",
        "- **Lasso** puede fallar con m√∫ltiples variables importantes\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMh1xC93FL7S"
      },
      "source": [
        "## T√©cnicas Avanzadas: Validaci√≥n Cruzada y Grid Search\n",
        "\n",
        "### ¬øPor qu√© Validaci√≥n Cruzada?\n",
        "- **Problema**: Una sola divisi√≥n train/test puede ser sesgada\n",
        "- **Soluci√≥n**: K-Fold CV divide datos en K partes, entrena K veces\n",
        "- **Beneficio**: Estimaci√≥n m√°s robusta del rendimiento\n",
        "\n",
        "### Grid Search con CV\n",
        "```python\n",
        "param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\n",
        "grid_search = GridSearchCV(Ridge(), param_grid, cv=5)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "```\n",
        "\n",
        "### Resultados de CV (5-fold)\n",
        "- **Linear Regression**: RMSE = 0.0006 ¬± 0.0004\n",
        "- **Ridge (optimizado)**: RMSE = 0.0006 ¬± 0.0005  \n",
        "- **Random Forest**: RMSE = 0.0006 ¬± 0.0005\n",
        "\n",
        "### Interpretaci√≥n\n",
        "- **¬±** indica **variabilidad** entre folds (menor = m√°s estable)\n",
        "- **Todos los modelos** muestran rendimiento similar y estable\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV9vS4eDFL7S"
      },
      "source": [
        "## M√©tricas de Evaluaci√≥n en Regresi√≥n\n",
        "\n",
        "### Principales M√©tricas Utilizadas\n",
        "\n",
        "#### 1. **RMSE (Root Mean Square Error)**\n",
        "\n",
        "- **F√≥rmula**: $\\sqrt{\\sum (y_\\textsf{real} - y_\\textsf{pred})^2 / n}$\n",
        "- **Ventajas**: Mismas unidades que variable objetivo, penaliza errores grandes\n",
        "- **Interpretaci√≥n**: Menor = mejor\n",
        "\n",
        "#### 2. **R¬≤ (Coeficiente de Determinaci√≥n)**\n",
        "\n",
        "- **Rango**: 0 a 1 (puede ser negativo si modelo es muy malo)\n",
        "- **Interpretaci√≥n**: % de varianza explicada por el modelo\n",
        "- **R¬≤ = 0.82** ‚Üí Modelo explica 82% de la variabilidad\n",
        "\n",
        "#### 3. **MAE (Mean Absolute Error)**\n",
        "\n",
        "- **F√≥rmula**: $\\sum |y_\\textsf{real} - y_\\textsf{pred}| / n$\n",
        "- **Ventaja**: Menos sensible a outliers que RMSE\n",
        "- **Uso**: Complementa RMSE para an√°lisis completo\n",
        "\n",
        "### An√°lisis de Residuos\n",
        "\n",
        "- **Residuos**: $y_\\textsf{real} - y_\\textsf{pred}$\n",
        "- **Ideal**: Distribuidos aleatoriamente alrededor de 0\n",
        "- **Problemas**: Patrones indican falta de ajuste del modelo\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCOP_VErFL7S"
      },
      "source": [
        "## Comparaci√≥n de Algoritmos de Regresi√≥n\n",
        "\n",
        "### **Linear Regression**\n",
        "- ‚úì Simple e interpretable\n",
        "- ‚úì R√°pido de entrenar\n",
        "- ‚úï Asume relaci√≥n lineal\n",
        "- ‚úï Sensible a outliers y multicolinealidad\n",
        "\n",
        "### **Ridge Regression (L2)**\n",
        "- ‚úì Controla sobreajuste con regularizaci√≥n  \n",
        "- ‚úì Maneja multicolinealidad\n",
        "- ‚úì Todos los coeficientes se mantienen (‚â† 0)\n",
        "- ‚úï No elimina variables irrelevantes\n",
        "\n",
        "### **Lasso Regression (L1)**  \n",
        "- ‚úì Selecci√≥n autom√°tica de variables (coef ‚Üí 0)\n",
        "- ‚úì Produce modelos m√°s simples\n",
        "- ‚úï Puede eliminar variables importantes\n",
        "- ‚úï Inestable con variables correlacionadas\n",
        "\n",
        "### **Random Forest**\n",
        "- ‚úì Maneja relaciones no lineales\n",
        "- ‚úì Robusto a outliers\n",
        "- ‚úì No requiere estandarizaci√≥n\n",
        "- ‚úï Menos interpretable (\"caja negra\")\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3Pns2R5FL7T"
      },
      "source": [
        "## Conclusiones y Recomendaciones\n",
        "\n",
        "### üéØ **Principales Aprendizajes**\n",
        "\n",
        "1. **Preprocesamiento es cr√≠tico**\n",
        "   - Estandarizaci√≥n obligatoria para Ridge/Lasso\n",
        "   - An√°lisis exploratorio revela patrones importantes\n",
        "\n",
        "2. **No hay modelo universalmente mejor**\n",
        "   - Linear: Simplicidad e interpretabilidad\n",
        "   - Random Forest: Mejor rendimiento en datos complejos  \n",
        "   - Ridge: Equilibrio entre sesgo y varianza\n",
        "\n",
        "3. **Validaci√≥n cruzada es esencial**\n",
        "   - Una sola divisi√≥n train/test puede enga√±ar\n",
        "   - Grid Search automatiza optimizaci√≥n de hiperpar√°metros\n",
        "\n",
        "### üí° **Recomendaciones Pr√°cticas**\n",
        "\n",
        "- **Empezar simple**: Linear Regression como baseline\n",
        "- **Probar regularizaci√≥n**: Ridge si hay multicolinealidad  \n",
        "- **Considerar ensemble**: Random Forest para relaciones complejas\n",
        "- **Siempre validar**: Cross-validation + m√©tricas m√∫ltiples\n",
        "- **Interpretar residuos**: Detecta problemas del modelo\n",
        "\n",
        "### üöÄ **Pr√≥ximos Pasos**\n",
        "- Explorar otros algoritmos (SVM, Neural Networks)\n",
        "- T√©cnicas de feature engineering m√°s avanzadas\n",
        "- Optimizaci√≥n bayesiana de hiperpar√°metros\n",
        "\n",
        "---\n",
        "\n",
        "### ¬°Gracias por su atenci√≥n!\n",
        "### ¬øPreguntas?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Ambiente ML",
      "language": "python",
      "name": "ml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
