# Machine Learning para Docentes

El contenido se enfoca en los **prerrequisitos esenciales** para abordar, en futuras sesiones, temas m谩s avanzados como **Redes Neurales de Aprendizaje Profundo (DNN)** y **Visi贸n Computacional**. La capacitaci贸n se estructura en dos componentes clave para asegurar un aprendizaje integral: **teor铆a y pr谩ctica.**

##  Inicio R谩pido

### Requisitos Previos

- Python 3.8 o superior
- pip (gestor de paquetes de Python)

### Configuraci贸n del Entorno

#### Opci贸n 1: Script Automatizado (Recomendado)

```bash
# Ejecutar el script de configuraci贸n
./setup.sh
```

#### Opci贸n 2: Configuraci贸n Manual

```bash
# Crear entorno virtual
python3 -m venv venv

# Activar entorno virtual
source venv/bin/activate  # En macOS/Linux
# o
venv\Scripts\activate  # En Windows

# Instalar dependencias
pip install -r requirements.txt
```

### Ejecutar los Notebooks

```bash
# Activar el entorno virtual (si no est谩 activado)
source venv/bin/activate

# Iniciar Jupyter Notebook
jupyter notebook

# O iniciar Jupyter Lab
jupyter lab
```

##  Materiales del Curso

- **ml-pca-apps.ipynb**: Notebook con aplicaciones pr谩cticas de PCA en Ingenier铆a Electr贸nica
- **contenido.pdf**: Material te贸rico del curso
- **ml-pca.pdf**: Documento de referencia sobre PCA

##  Dependencias Principales

- NumPy: C谩lculo num茅rico
- Pandas: Manipulaci贸n de datos
- Matplotlib: Visualizaci贸n
- Scikit-learn: Machine Learning
- Jupyter: Entorno interactivo

## Contenido

A continuaci贸n, se presenta la lista de temas propuestos y el n煤mero de horas asignadas para el desarrollo del curso, conforme a las conversaciones previas.

| Aula  | Tema                                                                                        |
| :---: | :-------------------------------------------------------------------------------------------|
|   1   | Python. An谩lisis de Componentes Principales.                                                |
|   2   | Python. Regresi贸n y selecci贸n de modelos.                                                   |
|   3   | Aprendizaje no supervisado: Agrupamiento *k-means*, Agrupamiento jer谩rquico *dendrograma.*  |
|   4   | Aprendizaje supervisado: Discriminantes lineales, M谩quinas de Vectores de Soporte.          |
|   5   | rboles de Clasificaci贸n y Bosques Aleatorios                                               |
|   6   | Redes neuronales artificiales                                                               |
|   7   | Aprendizaje profundo. Capas convolucionales y capas recurrentes                             |
|   8   | *Transformers*                                                                              |

---

### 1: Reducci贸n de Dimensionalidad

* **Descomposici贸n en Valores Singulares (SVD)**: Es una de las factorizaciones de matrices m谩s importantes. Se utiliza para obtener aproximaciones de bajo rango y para el c谩lculo de pseudoinversas.
* **An谩lisis de Componentes Principales (PCA)**: Un uso clave de la SVD es el algoritmo de PCA, que descompone datos de alta dimensi贸n en factores estad铆sticamente descriptivos.
* **Aplicaci贸n**: SVD y PCA se aplican en una amplia variedad de problemas de ciencia e ingenier铆a.

### 2: Regresi贸n y Selecci贸n de Modelos

El **aprendizaje autom谩tico** se centra en la optimizaci贸n. Herramientas de **regresi贸n y selecci贸n de modelos** permiten crear modelos interpretables, simples y eficientes. Cuando el modelo ideal no est谩 predefinido, se aplican m茅todos de optimizaci贸n para seleccionar el mejor.

### 3: Aprendizaje No Supervisado

El objetivo es agrupar datos sin etiquetas.

* **Agrupamiento K-Means**: Particiona un conjunto de *m* observaciones vectoriales en *k* grupos. El n煤mero de particiones (*k*) generalmente es desconocido y debe determinarse.
* **Agrupamiento Jer谩rquico (Dendrograma)**: Esta t茅cnica agrupa datos seg煤n su similitud, creando una jerarqu铆a en forma de 谩rbol. Comienza con cada observaci贸n como un grupo separado y las fusiona progresivamente.

\newpage

### 4: Aprendizaje Supervisado

En este tipo de aprendizaje, los datos etiquetados gu铆an el algoritmo de clasificaci贸n.

* **An谩lisis Discriminante Lineal (LDA)**: Una t茅cnica de clasificaci贸n est谩ndar cuyo objetivo es encontrar una combinaci贸n lineal de caracter铆sticas que separe dos o m谩s clases de objetos.
* **M谩quinas de Vectores de Soporte (SVM)**: Algoritmo de clasificaci贸n que encuentra un hiperplano 贸ptimo para maximizar la distancia entre las clases en un espacio N-dimensional.

### 5: rboles de Decisi贸n y Bosques Aleatorios

* **rbol de Decisi贸n**: Estructura jer谩rquica supervisada que divide los datos para lograr una clasificaci贸n y regresi贸n robustas. Al ser supervisado, utiliza datos etiquetados para optimizar las divisiones.
* **Bosques Aleatorios**: M茅todo de aprendizaje en conjunto que crea m煤ltiples 谩rboles de decisi贸n para mejorar la robustez. Es una mejora significativa, ya que los 谩rboles individuales pueden ser sensibles a las variaciones en los datos de entrenamiento.

### 6: Redes Neurales Artificiales (ANN)

Las **Redes Neuronales Artificiales** son sistemas de aprendizaje compuestos por neuronas interconectadas jer谩rquicamente. El aprendizaje se logra ajustando los "pesos sin谩pticos" para minimizar una funci贸n de costo. El algoritmo de **retropropagaci贸n** fue un avance crucial para entrenar estas redes de forma eficiente.

### 7: Aprendizaje Profundo (Deep Learning)

Las **redes neuronales profundas** se componen de m煤ltiples capas de neuronas interconectadas que refinan y optimizan las predicciones o clasificaciones.

* **Redes Neuronales Convolucionales (CNN)**: Ideales para visi贸n por computadora. Detectan caracter铆sticas y patrones en im谩genes para tareas como detecci贸n de objetos y reconocimiento de patrones.
* **Redes Neuronales Recurrentes (RNN)**: Utilizadas para datos secuenciales como series de tiempo o texto. Pueden procesar datos de forma secuencial y son comunes en el procesamiento del lenguaje natural y el reconocimiento de voz. Se identifican por sus bucles de retroalimentaci贸n.

### 8: Transformers

* **Transformers**: Arquitectura de red neuronal que ha revolucionado el Procesamiento del Lenguaje Natural (NLP).
* **Mecanismo de Autoatenci贸n**: Su principal innovaci贸n es la capacidad de capturar dependencias globales entre los elementos de una secuencia de entrada.
* **Ventaja**: Pueden procesar datos en paralelo, lo que los hace altamente eficientes y escalables para el entrenamiento en hardware moderno.

<center>

#### Dr. Cs. Miguel Pari SotoyDr. Cs. Claver Pari Soto

</center>
