{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "VM3u54NNu4MC",
        "outputId": "8a2a8542-353d-4ae3-8baf-ac84471e6b1e"
      },
      "source": [
        "# Máquinas de Vectores de Soporte (SVM)\n",
        "## Presentación - Encuentro 4\n",
        "\n",
        "---\n",
        "\n",
        "## 1. ¿Qué es SVM?\n",
        "\n",
        "### Concepto Simple\n",
        "Las **Máquinas de Vectores de Soporte (SVM)** son algoritmos que encuentran la **mejor manera de separar** diferentes grupos de datos.\n",
        "\n",
        "### Analogía Visual\n",
        "Imagine que tiene **perros** y **gatos** mezclados en un patio. El SVM encuentra la **línea más amplia** entre ellos para separarlos claramente.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Los 4 Conceptos Fundamentales\n",
        "\n",
        "### 2.1 Hiperplano\n",
        "- **Qué es**: La línea (o plano) que separa las clases\n",
        "- **Objetivo**: Buscar la línea que esté lo más lejos posible de ambos grupos\n",
        "- **Visualización**:\n",
        "  - 2D: una línea recta\n",
        "  - 3D: un plano\n",
        "  - nD: un hiperplano (generalización)\n",
        "\n",
        "    ![](ml-svm-clasificador-c.png)\n",
        "\n",
        "    [Imagen extraida de Luis G. Serrano - Machine Learning]()\n",
        "\n",
        "### 2.2 Vectores de Soporte\n",
        "- **Qué son**: Los puntos de datos **más cercanos** a la línea de separación\n",
        "- **Importancia**: Solo necesitas estos puntos para clasificar nuevos datos\n",
        "- **Ventaja**: Muy eficiente en memoria\n",
        "\n",
        "  **Ejemplo**: 1000 muestras $\\to$ 50 vectores (puntos) de soporte\n",
        "\n",
        "### 2.3 Margen\n",
        "- **Qué es**: El espacio libre entre la línea de separación y los puntos más cercanos\n",
        "- **Objetivo**: Maximizar este espacio para crear una separación más robusta\n",
        "- **Tipos**:\n",
        "  - **Margen duro**: No permite errores\n",
        "  - **Margen suave**: Permite algunos errores (más flexible)\n",
        "\n",
        "    ![](ml-svm-clasificador-a.png)\n",
        "\n",
        "    [Imagen extraida de Luis G. Serrano - Machine Learning]()\n",
        "\n",
        "### 2.4 Kernel\n",
        "- **Qué es**: Una función que permite separaciones **no lineales** (curvas)\n",
        "- **Tipos principales**:\n",
        "  - **Lineal**: Separaciones rectas (más rápido)\n",
        "  - **RBF**: Separaciones curvas complejas (más flexible)\n",
        "  - **Polinomial**: Separaciones con curvas polinómicas\n",
        "- **Cuándo usar**: RBF para casos generales\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Parámetros Principales\n",
        "\n",
        "### Parámetro C (Regularización)\n",
        "| Valor             | Significado                            | Uso Recomendado               |\n",
        "|-------------------|----------------------------------------|-------------------------------|\n",
        "| C grande (>100)   | Línea rígida, pocos errores permitidos | Cuando quieres alta precisión |\n",
        "| C pequeño (<1)    | Línea flexible, más errores permitidos | Cuando los datos tienen ruido |\n",
        "| **Valor inicial** | C=1                                    | Punto de partida estándar     |\n",
        "\n",
        "### Parámetro gamma (Solo para kernel RBF)\n",
        "| Valor                | Significado          | Uso Recomendado         |\n",
        "|----------------------|----------------------|-------------------------|\n",
        "| gamma grande (>1)    | Curvas muy complejas | Riesgo de sobreajuste   |\n",
        "| gamma pequeño (<0.1) | Curvas suaves        | Mejor generalización    |\n",
        "| **Valor inicial**    | gamma=0.1            | Valor seguro y estándar |\n",
        "\n",
        "### Grid Search\n",
        "- **Qué es**: Prueba automáticamente diferentes combinaciones de parámetros\n",
        "- **Ventaja**: Encuentra la mejor combinación automáticamente\n",
        "- **Resultado**: Te entrega el modelo ya optimizado\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Flujo de Trabajo con SVM\n",
        "\n",
        "### Paso a Paso\n",
        "\n",
        "1. **Preparar datos**\n",
        "   - Obtener los datos\n",
        "   - Características y etiquetas\n",
        "  \n",
        "2. **Dividir datos**\n",
        "   - 80% para entrenamiento\n",
        "   - 20% para prueba\n",
        "\n",
        "3. **Normalizar datos**\n",
        "   - **CRÍTICO**: Normalizar con StandardScaler\n",
        "   - SVM es muy sensible a la escala de los datos\n",
        "\n",
        "4. **Grid Search**\n",
        "   - Prueba diferentes combinaciones automáticamente\n",
        "   - Ya entrena el modelo con los mejores parámetros\n",
        "\n",
        "5. **Evaluar y visualizar**\n",
        "   - Ver las fronteras de decisión\n",
        "   - Calcular métricas de rendimiento\n",
        "\n",
        "### Ejemplo: Comparación de SVM con GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SVM (lineal, RBF y polinomial), GridSearchCV, 5-Fold y PCA para visualización\n",
        "# Incluye simulación de uso en producción y matriz de confusión en porcentajes\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "import joblib\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Generar datos sintéticos\n",
        "X, y = make_blobs(n_samples=1000, centers=4, n_features=10, cluster_std=5, random_state=42)\n",
        "\n",
        "# Dividir en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalizar los datos\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Definir la grilla de hiperparámetros\n",
        "param_grid = [\n",
        "    {'kernel': ['linear'], 'C': [0.1, 1, 10]},\n",
        "    {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': ['scale', 0.1, 1]},\n",
        "    {'kernel': ['poly'], 'C': [0.1, 1, 10], 'degree': [2, 3, 4], 'gamma': ['scale', 0.1, 1]}\n",
        "]\n",
        "\n",
        "# Crear el modelo base y el GridSearchCV con validación cruzada (5 folds)\n",
        "svm = SVC()\n",
        "grid = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Mostrar los mejores parámetros\n",
        "print(\"Mejor combinación y puntuación encontrada por GridSearchCV:\")\n",
        "print(f\"{grid.best_params_}, {grid.best_score_:.3f}\")\n",
        "\n",
        "# Evaluar el mejor modelo\n",
        "best_svm = grid.best_estimator_\n",
        "y_pred = best_svm.predict(X_test_scaled)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Precisión en el conjunto de prueba: {acc:.3f}\")\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Matriz de confusión en valores absolutos\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nMatriz de confusión (valores absolutos):\")\n",
        "print(cm)\n",
        "\n",
        "# Matriz de confusión en porcentajes\n",
        "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "print(\"\\nMatriz de confusión (porcentajes):\")\n",
        "# Formato directo evitando notación científica\n",
        "np.set_printoptions(precision=2, suppress=True, floatmode='fixed')\n",
        "print(cm_percent)\n",
        "\n",
        "graficar_matriz_de_confusion = False\n",
        "if graficar_matriz_de_confusion:\n",
        "    # Visualización de la matriz de confusión\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Matriz de confusión absoluta\n",
        "    ConfusionMatrixDisplay.from_estimator(best_svm, X_test_scaled, y_test, cmap='Blues', ax=ax1)\n",
        "    ax1.set_title(\"Matriz de confusión (valores absolutos)\")\n",
        "    ax1.set_xlabel(\"Predicción\")\n",
        "    ax1.set_ylabel(\"Valor real\")\n",
        "\n",
        "    # Matriz de confusión en porcentajes\n",
        "    ConfusionMatrixDisplay(confusion_matrix=cm_percent, display_labels=best_svm.classes_).plot(ax=ax2, cmap='Blues', values_format='.2f')\n",
        "    ax2.set_title(\"Matriz de confusión (porcentajes)\")\n",
        "    ax2.set_xlabel(\"Predicción\")\n",
        "    ax2.set_ylabel(\"Valor real\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "graficar_fronteras_de_decision = False\n",
        "if graficar_fronteras_de_decision:\n",
        "    # Reducción a 2D con PCA (solo para visualización)\n",
        "    if X_train_scaled.shape[1] > 2:\n",
        "        print(f\"\\nAplicando PCA: reducción de {X_train_scaled.shape[1]} → 2 dimensiones para visualización.\")\n",
        "        pca = PCA(n_components=2)\n",
        "        X_vis = pca.fit_transform(X_train_scaled)\n",
        "    else:\n",
        "        X_vis = X_train_scaled  # si ya tiene 2 dimensiones, no aplicamos PCA\n",
        "\n",
        "    # Visualización de fronteras de decisión (no se usa para entrenar ni usar el modelo)\n",
        "    plt.figure(figsize=(8,6))\n",
        "\n",
        "    # Crear una malla 2D\n",
        "    x_min, x_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1\n",
        "    y_min, y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
        "                        np.linspace(y_min, y_max, 300))\n",
        "\n",
        "    # Si los datos fueron reducidos con PCA, proyectamos la malla al espacio original para predecir\n",
        "    if X_train_scaled.shape[1] > 2:\n",
        "        # Invertimos la proyección PCA para volver al espacio escalado\n",
        "        grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "        grid_points_scaled = pca.inverse_transform(grid_points)\n",
        "        Z = best_svm.predict(grid_points_scaled)\n",
        "    else:\n",
        "        Z = best_svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Dibujar fronteras\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='Accent')\n",
        "    plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y_train, cmap='Accent', edgecolors='k')\n",
        "    plt.title(f\"Fronteras de decisión del mejor SVM ({grid.best_params_['kernel']})\")\n",
        "    plt.xlabel(\"Componente 1 (PCA)\" if X_train_scaled.shape[1] > 2 else \"Característica 1\")\n",
        "    plt.ylabel(\"Componente 2 (PCA)\" if X_train_scaled.shape[1] > 2 else \"Característica 2\")\n",
        "    plt.show()\n",
        "\n",
        "# Guardar el modelo entrenado y el scaler para uso en producción\n",
        "joblib.dump(best_svm, 'svm_model.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "print(\"\\nModelo y scaler guardados para uso en producción:\")\n",
        "print(\"- svm_model.pkl\")\n",
        "print(\"- scaler.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ¿Cuándo Usar SVM?\n",
        "\n",
        "### Ventajas\n",
        "- **Eficiente en memoria**: Solo almacena vectores de soporte\n",
        "- **Bueno con alta dimensionalidad**: Funciona bien con muchas características\n",
        "- **Flexible**: Diferentes kernels para diferentes tipos de separación\n",
        "- **Robusto**: Resiste datos atípicos\n",
        "\n",
        "### Desventajas\n",
        "- **Lento con datasets muy grandes** (>10,000 muestras)\n",
        "- **Requiere normalización** (no opcional)\n",
        "- **Parámetros sensibles** (requieren ajuste)\n",
        "\n",
        "### Cuándo Usar\n",
        "- Dataset pequeño a mediano (<10,000 muestras)\n",
        "- Necesitas separaciones no lineales\n",
        "- Trabajas con alta dimensionalidad\n",
        "- Necesitas un modelo robusto\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Métricas de Evaluación\n",
        "\n",
        "### Matriz de Confusión\n",
        "\n",
        "| Real \\ Predicción | Positiva        | Negativa        |\n",
        "|-------------------|-----------------|-----------------|\n",
        "| **Positiva**      | **VP** o **TP** | FN              |\n",
        "| **Negativa**      | FP              | **VN** o **TN** |\n",
        "\n",
        "#### Ejemplo:\n",
        "\n",
        "| Real \\ Predicho | Perro       | Gato        | Pájaro      |\n",
        "|-----------------|-------------|-------------|-------------|\n",
        "| **Perro**       | **70 (TP)** | 5 (FN)      | 2 (FN)      |\n",
        "| **Gato**        | 10 (FP)     | **85 (TP)** | 5           |\n",
        "| **Pájaro**      | 0 (FP)      | 3           | **95 (TP)** |\n",
        "\n",
        "\n",
        "### Métricas Principales\n",
        "\n",
        "**Fórmulas básicas:**\n",
        "- **Accuracy**: (TP + TN) / Total\n",
        "- **Precision**: TP / (TP + FP)  \n",
        "- **Recall**: TP / (TP + FN)\n",
        "- **F1-Score**: 2 × (Precision × Recall) / (Precision + Recall)\n",
        "\n",
        "| Métrica       | Interpretación                                |\n",
        "|---------------|-----------------------------------------------|\n",
        "| **Accuracy**  | Porcentaje de clasificaciones correctas      |\n",
        "| **Precision** | De los predichos positivos, ¿cuántos son realmente positivos? |\n",
        "| **Recall**    | De los realmente positivos, ¿cuántos detectamos? |\n",
        "| **F1-Score**  | Balancea precisión y recall (media armónica)  |\n",
        "\n",
        "### ¿Qué Métrica Usar?\n",
        "\n",
        "| Escenario                    | Métrica Principal | Razón                           |\n",
        "|------------------------------|-------------------|---------------------------------|\n",
        "| Clases balanceadas           | **Accuracy**      | Representa bien el rendimiento  |\n",
        "| Clase minoritaria importante | **F1-Score**      | Balancea ambas métricas         |\n",
        "| Evitar falsas alarmas        | **Precision**     | Ej: spam vs no-spam             |\n",
        "| Detectar todos los casos     | **Recall**        | Ej: detección de fallas, cáncer |\n",
        "| Clases desbalanceadas        | **Macro F1**      | No se sesga por mayoría         |\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Mejores Prácticas\n",
        "\n",
        "### Hacer SIEMPRE\n",
        "1. **Normalizar datos** (crítico)\n",
        "2. Usar **Grid Search** para encontrar mejores parámetros\n",
        "3. **Validación cruzada** para evaluar robustez\n",
        "4. Comparar **train vs test accuracy** para detectar sobreajuste\n",
        "5. Empezar con valores conservadores de C y gamma\n",
        "\n",
        "### Errores Comunes\n",
        "1. **No normalizar** datos de prueba correctamente\n",
        "2. **Usar datos de prueba durante entrenamiento**\n",
        "3. **Ignorar sobreajuste** (train accuracy alta, test baja)\n",
        "4. **Valores extremos** de C y gamma sin validar\n",
        "5. **No visualizar** las fronteras de decisión\n",
        "\n",
        "### Interpretación de Resultados\n",
        "- **Pocos vectores de soporte**: Modelo simple y generalizable\n",
        "- **Muchos vectores de soporte**: Modelo complejo (posible sobreajuste)\n",
        "- **Train accuracy alta, test baja**: Sobreajuste (reducir C o gamma)\n",
        "- **Baja accuracy en ambos**: Bajo rendimiento (cambiar kernel o parámetros)\n",
        "\n",
        "---\n",
        "\n",
        "## 8. El Camino de los Datos: Entrenamiento → Predicción\n",
        "\n",
        "  ![](ml-svm-entren-pred.svg)\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Entrenamiento del modelo\n",
        "\n",
        "  ![](ml-svm-entren-modelo.svg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Función para inspeccionar el modelo cargado\n",
        "def inspect_model():\n",
        "    \"\"\"\n",
        "    Inspecciona el modelo SVM cargado y muestra información detallada\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"INSPECCIÓN DEL MODELO SVM\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Cargar el modelo\n",
        "    model = joblib.load('svm_model.pkl')\n",
        "\n",
        "    # Información básica del modelo\n",
        "    print(f\"Parámetros del modelo  {type(model).__name__}:\")\n",
        "    print(f\"  - Kernel: {model.kernel}\")\n",
        "    print(f\"  - C: {model.C}\")\n",
        "    print(f\"  - Gamma: {model.gamma}\")\n",
        "\n",
        "    # Información del entrenamiento\n",
        "    print(f\"\\nInformación del entrenamiento:\")\n",
        "    print(f\"  - Número de clases y clases:     {len(model.classes_)} - {model.classes_}\")\n",
        "    print(f\"  - Número de características:     {model.n_features_in_}\")\n",
        "    print(f\"  - Número de vectores de soporte: {model.n_support_}\")\n",
        "    print(f\"  - Vectores de soporte por clase: {model.n_support_}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Función para simular predicciones en producción\n",
        "def predict_production_samples(new_samples):\n",
        "    \"\"\"\n",
        "    Simula el uso del modelo en producción con nuevas muestras\n",
        "\n",
        "    Args:\n",
        "        new_samples: array o DataFrame con las nuevas muestras a predecir\n",
        "\n",
        "    Returns:\n",
        "        predictions: array con las predicciones\n",
        "        probabilities: array con las probabilidades de cada clase\n",
        "    \"\"\"\n",
        "    # Cargar el modelo y scaler guardados\n",
        "    model = joblib.load('svm_model.pkl')\n",
        "    scaler_loaded = joblib.load('scaler.pkl')\n",
        "\n",
        "    # Normalizar las nuevas muestras\n",
        "    new_samples_scaled = scaler_loaded.transform(new_samples)\n",
        "\n",
        "    # Hacer predicciones\n",
        "    predictions = model.predict(new_samples_scaled)\n",
        "\n",
        "    # Obtener probabilidades (si el modelo las soporta)\n",
        "    try:\n",
        "        probabilities = model.predict_proba(new_samples_scaled)\n",
        "    except AttributeError:\n",
        "        # Para SVM sin probabilidades, usar decision_function\n",
        "        decision_scores = model.decision_function(new_samples_scaled)\n",
        "        probabilities = decision_scores\n",
        "\n",
        "    return predictions, probabilities\n",
        "\n",
        "# Inspeccionar el modelo\n",
        "model = inspect_model()\n",
        "\n",
        "# Simulación de uso en producción\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SIMULACIÓN DE USO EN PRODUCCIÓN\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Generar nuevas muestras sintéticas para simular datos de producción\n",
        "# np.random.seed(123)\n",
        "n_samples = 20\n",
        "n_features = model.n_features_in_\n",
        "new_samples = np.random.randn(n_samples, n_features) * 3 + np.random.randn(n_samples, n_features) * 2\n",
        "\n",
        "print(f\"\\nPrediciendo {len(new_samples)} nuevas muestras con {n_features} características cada una...\")\n",
        "predictions, probabilities = predict_production_samples(new_samples)\n",
        "\n",
        "# Mostrar resultados\n",
        "print(\"\\nResultados de las predicciones:\")\n",
        "for i, (pred, prob) in enumerate(zip(predictions, probabilities)):\n",
        "    if len(prob.shape) > 1:  # Si tenemos probabilidades por clase\n",
        "        prob_str = f\"Probabilidades: {prob.round(3)}\"\n",
        "    else:                    # Si no tenemos probabilidades, tenemos decision scores\n",
        "        prob_str = f\"Decision scores: {prob.round(3)}\"\n",
        "\n",
        "    print(f\"Muestra {i+1:2d}: Clase predicha = {pred}, {prob_str}\")\n",
        "\n",
        "# Estadísticas de las predicciones\n",
        "unique, counts = np.unique(predictions, return_counts=True)\n",
        "print(f\"\\nDistribución de predicciones:\")\n",
        "for cls, count in zip(unique, counts):\n",
        "    percentage = (count / len(predictions)) * 100\n",
        "    print(f\"Clase {cls}: {count} muestras ({percentage:.1f}%)\")\n",
        "\n",
        "print(\"\\nSimulación completada exitosamente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Puntos Clave para Recordar\n",
        "\n",
        "### Los 3 Conceptos Esenciales\n",
        "1. **Hiperplano**: La línea que separa clases\n",
        "2. **Vectores de Soporte**: Puntos clave que definen la separación\n",
        "3. **Margen**: Espacio que queremos maximizar\n",
        "\n",
        "### Los 2 Parámetros Críticos\n",
        "1. **C**: Controla cuánto errores permitir (C grande = menos errores)\n",
        "2. **gamma**: Controla complejidad de curvas (gamma pequeño = más simple)\n",
        "\n",
        "### El Flujo Clave\n",
        "1. Normalizar (SIEMPRE)\n",
        "2. Grid Search (optimización automática)\n",
        "3. Visualizar (entender el modelo)\n",
        "4. Evaluar (métricas adecuadas)\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Próximos Pasos\n",
        "\n",
        "### Para Practicar\n",
        "1. Ejecutar el notebook de práctica\n",
        "2. Probar diferentes kernels (lineal, RBF)\n",
        "3. Cambiar parámetros C y gamma\n",
        "4. Aplicar a dataset real (ej: Iris)\n",
        "5. Experimentar con datos multiclase\n",
        "\n",
        "### Para Profundizar\n",
        "- Explorar SVM multiclase\n",
        "- Implementar en casos industriales reales\n",
        "- Comparar con otros algoritmos (Random Forest, Redes Neuronales)\n",
        "- Estudiar teoría matemática de SVM\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Recursos y Referencias\n",
        "\n",
        "### Documentación\n",
        "- Grokking Machine Learning, Luis G. Serrano, 2021\n",
        "- [Scikit-learn: SVM](https://scikit-learn.org/stable/modules/svm.html)\n",
        "- [Grid Search Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
        "\n",
        "### Para Aprender Más\n",
        "- Libro: \"Pattern Recognition and Machine Learning\" - Christopher Bishop\n",
        "- Curso: Machine Learning por Andrew Ng (Coursera)\n",
        "\n",
        "---\n",
        "\n",
        "## ¿Preguntas?\n",
        "\n",
        "*Tiempo restante para preguntas y discusión*\n",
        "\n",
        "---\n",
        "\n",
        "## Apéndice: Comparación Rápida con Otros Algoritmos\n",
        "\n",
        "| Algoritmo           | Interpretabilidad | Escalabilidad | Datos No Lineales | Velocidad |\n",
        "|---------------------|-------------------|---------------|-------------------|-----------|\n",
        "| **SVM**             | Media             | Baja          | Excelente         | Media     |\n",
        "| Random Forest       | Alta              | Alta          | Buena             | Alta      |\n",
        "| Redes Neuronales    | Baja              | Media         | Excelente         | Media     |\n",
        "| Regresión Logística | Alta              | Alta          | Limitada          | Alta      |\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
