{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "VM3u54NNu4MC",
        "outputId": "8a2a8542-353d-4ae3-8baf-ac84471e6b1e"
      },
      "source": [
        "# Máquinas de Vectores de Soporte (SVM)\n",
        "## Presentación - Encuentro 4\n",
        "\n",
        "---\n",
        "\n",
        "## 1. ¿Qué es SVM?\n",
        "\n",
        "### Concepto Simple\n",
        "Las **Máquinas de Vectores de Soporte (SVM)** son algoritmos que encuentran la **mejor manera de separar** diferentes grupos de datos.\n",
        "\n",
        "### Analogía Visual\n",
        "Imagine que tiene **perros** y **gatos** mezclados en un patio. El SVM encuentra la **línea más amplia** entre ellos para separarlos claramente.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Los 4 Conceptos Fundamentales\n",
        "\n",
        "### 2.1 Hiperplano\n",
        "- **Qué es**: La línea (o plano) que separa las clases\n",
        "- **Objetivo**: Buscar la línea que esté lo más lejos posible de ambos grupos\n",
        "- **Visualización**:\n",
        "  - 2D: una línea recta\n",
        "  - 3D: un plano\n",
        "  - nD: un hiperplano (generalización)\n",
        "\n",
        "    ![](svm-clasificador-c.png)\n",
        "\n",
        "    [Imagen extraida de Luis G. Serrano - Machine Learning]()\n",
        "\n",
        "### 2.2 Vectores de Soporte\n",
        "- **Qué son**: Los puntos de datos **más cercanos** a la línea de separación\n",
        "- **Importancia**: Solo necesitas estos puntos para clasificar nuevos datos\n",
        "- **Ventaja**: Muy eficiente en memoria\n",
        "\n",
        "  **Ejemplo**: 1000 muestras $\\to$ 50 vectores (puntos) de soporte\n",
        "\n",
        "### 2.3 Margen\n",
        "- **Qué es**: El espacio libre entre la línea de separación y los puntos más cercanos\n",
        "- **Objetivo**: Maximizar este espacio para crear una separación más robusta\n",
        "- **Tipos**:\n",
        "  - **Margen duro**: No permite errores\n",
        "  - **Margen suave**: Permite algunos errores (más flexible)\n",
        "\n",
        "    ![](svm-clasificador-a.png)\n",
        "\n",
        "    [Imagen extraida de Luis G. Serrano - Machine Learning]()\n",
        "\n",
        "### 2.4 Kernel\n",
        "- **Qué es**: Una función que permite separaciones **no lineales** (curvas)\n",
        "- **Tipos principales**:\n",
        "  - **Lineal**: Separaciones rectas (más rápido)\n",
        "  - **RBF**: Separaciones curvas complejas (más flexible)\n",
        "  - **Polinomial**: Separaciones con curvas polinómicas\n",
        "- **Cuándo usar**: RBF para casos generales\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Parámetros Principales\n",
        "\n",
        "### Parámetro C (Regularización)\n",
        "| Valor             | Significado                            | Uso Recomendado               |\n",
        "|-------------------|----------------------------------------|-------------------------------|\n",
        "| C grande (>100)   | Línea rígida, pocos errores permitidos | Cuando quieres alta precisión |\n",
        "| C pequeño (<1)    | Línea flexible, más errores permitidos | Cuando los datos tienen ruido |\n",
        "| **Valor inicial** | C=1                                    | Punto de partida estándar     |\n",
        "\n",
        "### Parámetro gamma (Solo para kernel RBF)\n",
        "| Valor                | Significado          | Uso Recomendado         |\n",
        "|----------------------|----------------------|-------------------------|\n",
        "| gamma grande (>1)    | Curvas muy complejas | Riesgo de sobreajuste   |\n",
        "| gamma pequeño (<0.1) | Curvas suaves        | Mejor generalización    |\n",
        "| **Valor inicial**    | gamma=0.1            | Valor seguro y estándar |\n",
        "\n",
        "### Grid Search\n",
        "- **Qué es**: Prueba automáticamente diferentes combinaciones de parámetros\n",
        "- **Ventaja**: Encuentra la mejor combinación automáticamente\n",
        "- **Resultado**: Te entrega el modelo ya optimizado\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Flujo de Trabajo con SVM\n",
        "\n",
        "### Paso a Paso\n",
        "\n",
        "1. **Preparar datos**\n",
        "   - **CRÍTICO**: Normalizar con StandardScaler\n",
        "   - SVM es muy sensible a la escala de los datos\n",
        "\n",
        "2. **Dividir datos**\n",
        "   - 80% para entrenamiento\n",
        "   - 20% para prueba\n",
        "\n",
        "3. **Grid Search**\n",
        "   - Prueba diferentes combinaciones automáticamente\n",
        "   - Ya entrena el modelo con los mejores parámetros\n",
        "\n",
        "4. **Evaluar y visualizar**\n",
        "   - Ver las fronteras de decisión\n",
        "   - Calcular métricas de rendimiento\n",
        "\n",
        "### Ejemplo: Comparación de SVM con GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SVM (lineal, RBF y polinomial), GridSearchCV, 5-Fold y PCA para visualización\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 1. Generar datos sintéticos (puedes cambiar n_features > 2 para probar PCA)\n",
        "X, y = make_blobs(n_samples=400, centers=2, n_features=5, cluster_std=4, random_state=42)\n",
        "\n",
        "# 2. Dividir en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# 3. Definir la grilla de hiperparámetros\n",
        "param_grid = [\n",
        "    {'kernel': ['linear'], 'C': [0.1, 1, 10]},\n",
        "    {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': ['scale', 0.1, 1]},\n",
        "    {'kernel': ['poly'], 'C': [0.1, 1, 10], 'degree': [2, 3, 4], 'gamma': ['scale', 0.1, 1]}\n",
        "]\n",
        "\n",
        "# 4. Crear el modelo base y el GridSearchCV con validación cruzada (5 folds)\n",
        "svm = SVC()\n",
        "grid = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 5. Mostrar los mejores parámetros\n",
        "print(\"Mejor combinación encontrada por GridSearchCV:\")\n",
        "print(grid.best_params_)\n",
        "print(f\"Mejor puntuación media de validación: {grid.best_score_:.3f}\")\n",
        "\n",
        "# 6. Evaluar el mejor modelo\n",
        "best_svm = grid.best_estimator_\n",
        "y_pred = best_svm.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nPrecisión en el conjunto de prueba: {acc:.3f}\")\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 7. Matriz de confusión\n",
        "ConfusionMatrixDisplay.from_estimator(best_svm, X_test, y_test, cmap='Blues')\n",
        "plt.title(\"Matriz de confusión del mejor SVM\")\n",
        "plt.show()\n",
        "\n",
        "# 8. Reducción a 2D con PCA (solo para visualización)\n",
        "if X.shape[1] > 2:\n",
        "    print(f\"\\nAplicando PCA: reducción de {X.shape[1]} → 2 dimensiones para visualización.\")\n",
        "    pca = PCA(n_components=2)\n",
        "    X_vis = pca.fit_transform(X)\n",
        "else:\n",
        "    X_vis = X  # si ya tiene 2 dimensiones, no aplicamos PCA\n",
        "\n",
        "# 9. Visualización de fronteras de decisión\n",
        "# Usamos PCA solo para graficar, no para entrenar el modelo (por claridad)\n",
        "# En escenarios reales, puedes aplicar PCA antes del entrenamiento si lo deseas.\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "# Crear una malla 2D\n",
        "x_min, x_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1\n",
        "y_min, y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
        "                     np.linspace(y_min, y_max, 300))\n",
        "\n",
        "# Si los datos fueron reducidos con PCA, proyectamos la malla al espacio original para predecir\n",
        "if X.shape[1] > 2:\n",
        "    # Invertimos la proyección para estimar el espacio original\n",
        "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    grid_points_original = pca.inverse_transform(grid_points)\n",
        "    Z = best_svm.predict(grid_points_original)\n",
        "else:\n",
        "    Z = best_svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Dibujar fronteras\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='Accent')\n",
        "plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y, cmap='Accent', edgecolors='k')\n",
        "plt.title(f\"Fronteras de decisión del mejor SVM ({grid.best_params_['kernel']})\")\n",
        "plt.xlabel(\"Componente 1 (PCA)\" if X.shape[1] > 2 else \"Característica 1\")\n",
        "plt.ylabel(\"Componente 2 (PCA)\" if X.shape[1] > 2 else \"Característica 2\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ¿Cuándo Usar SVM?\n",
        "\n",
        "### Ventajas\n",
        "- **Eficiente en memoria**: Solo almacena vectores de soporte\n",
        "- **Bueno con alta dimensionalidad**: Funciona bien con muchas características\n",
        "- **Flexible**: Diferentes kernels para diferentes tipos de separación\n",
        "- **Robusto**: Resiste datos atípicos\n",
        "\n",
        "### Desventajas\n",
        "- **Lento con datasets muy grandes** (>10,000 muestras)\n",
        "- **Requiere normalización** (no opcional)\n",
        "- **Parámetros sensibles** (requieren ajuste)\n",
        "\n",
        "### Cuándo Usar\n",
        "- Dataset pequeño a mediano (<10,000 muestras)\n",
        "- Necesitas separaciones no lineales\n",
        "- Trabajas con alta dimensionalidad\n",
        "- Necesitas un modelo robusto\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Métricas de Evaluación\n",
        "\n",
        "### Matriz de Confusión\n",
        "\n",
        "| Real \\ Predicción | Positiva        | Negativa        |\n",
        "|-------------------|-----------------|-----------------|\n",
        "| **Positiva**      | **VP** o **TP** | FN              |\n",
        "| **Negativa**      | FP              | **VN** o **TN** |\n",
        "\n",
        "#### Ejemplo:\n",
        "\n",
        "| Real \\ Predicho | Perro       | Gato        | Pájaro      |\n",
        "|-----------------|-------------|-------------|-------------|\n",
        "| **Perro**       | **70 (TP)** | 5 (FN)      | 2 (FN)      |\n",
        "| **Gato**        | 10 (FP)     | **85 (TP)** | 5           |\n",
        "| **Pájaro**      | 0 (FP)      | 3           | **95 (TP)** |\n",
        "\n",
        "\n",
        "### Métricas Principales\n",
        "\n",
        "**Fórmulas básicas:**\n",
        "- **Accuracy**: (TP + TN) / Total\n",
        "- **Precision**: TP / (TP + FP)  \n",
        "- **Recall**: TP / (TP + FN)\n",
        "- **F1-Score**: 2 × (Precision × Recall) / (Precision + Recall)\n",
        "\n",
        "| Métrica       | Interpretación                                |\n",
        "|---------------|-----------------------------------------------|\n",
        "| **Accuracy**  | Porcentaje de clasificaciones correctas      |\n",
        "| **Precision** | De los predichos positivos, ¿cuántos son realmente positivos? |\n",
        "| **Recall**    | De los realmente positivos, ¿cuántos detectamos? |\n",
        "| **F1-Score**  | Balancea precisión y recall (media armónica)  |\n",
        "\n",
        "### ¿Qué Métrica Usar?\n",
        "\n",
        "| Escenario                    | Métrica Principal | Razón                           |\n",
        "|------------------------------|-------------------|---------------------------------|\n",
        "| Clases balanceadas           | **Accuracy**      | Representa bien el rendimiento  |\n",
        "| Clase minoritaria importante | **F1-Score**      | Balancea ambas métricas         |\n",
        "| Evitar falsas alarmas        | **Precision**     | Ej: spam vs no-spam             |\n",
        "| Detectar todos los casos     | **Recall**        | Ej: detección de fallas, cáncer |\n",
        "| Clases desbalanceadas        | **Macro F1**      | No se sesga por mayoría         |\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Mejores Prácticas\n",
        "\n",
        "### Hacer SIEMPRE\n",
        "1. **Normalizar datos** (crítico)\n",
        "2. Usar **Grid Search** para encontrar mejores parámetros\n",
        "3. **Validación cruzada** para evaluar robustez\n",
        "4. Comparar **train vs test accuracy** para detectar sobreajuste\n",
        "5. Empezar con valores conservadores de C y gamma\n",
        "\n",
        "### Errores Comunes\n",
        "1. **No normalizar** datos de prueba correctamente\n",
        "2. **Usar datos de prueba durante entrenamiento**\n",
        "3. **Ignorar sobreajuste** (train accuracy alta, test baja)\n",
        "4. **Valores extremos** de C y gamma sin validar\n",
        "5. **No visualizar** las fronteras de decisión\n",
        "\n",
        "### Interpretación de Resultados\n",
        "- **Pocos vectores de soporte**: Modelo simple y generalizable\n",
        "- **Muchos vectores de soporte**: Modelo complejo (posible sobreajuste)\n",
        "- **Train accuracy alta, test baja**: Sobreajuste (reducir C o gamma)\n",
        "- **Baja accuracy en ambos**: Bajo rendimiento (cambiar kernel o parámetros)\n",
        "\n",
        "---\n",
        "\n",
        "## 8. El Camino de los Datos: Entrenamiento → Predicción\n",
        "\n",
        "  ![](ml-svm-blocos_diagram_01.png)\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Entrenamiento del modelo\n",
        "\n",
        "  ![](ml-svm-blocos_diagram_02.png)\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Puntos Clave para Recordar\n",
        "\n",
        "### Los 3 Conceptos Esenciales\n",
        "1. **Hiperplano**: La línea que separa clases\n",
        "2. **Vectores de Soporte**: Puntos clave que definen la separación\n",
        "3. **Margen**: Espacio que queremos maximizar\n",
        "\n",
        "### Los 2 Parámetros Críticos\n",
        "1. **C**: Controla cuánto errores permitir (C grande = menos errores)\n",
        "2. **gamma**: Controla complejidad de curvas (gamma pequeño = más simple)\n",
        "\n",
        "### El Flujo Clave\n",
        "1. Normalizar (SIEMPRE)\n",
        "2. Grid Search (optimización automática)\n",
        "3. Visualizar (entender el modelo)\n",
        "4. Evaluar (métricas adecuadas)\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Próximos Pasos\n",
        "\n",
        "### Para Practicar\n",
        "1. Ejecutar el notebook de práctica\n",
        "2. Probar diferentes kernels (lineal, RBF)\n",
        "3. Cambiar parámetros C y gamma\n",
        "4. Aplicar a dataset real (ej: Iris)\n",
        "5. Experimentar con datos multiclase\n",
        "\n",
        "### Para Profundizar\n",
        "- Explorar SVM multiclase\n",
        "- Implementar en casos industriales reales\n",
        "- Comparar con otros algoritmos (Random Forest, Redes Neuronales)\n",
        "- Estudiar teoría matemática de SVM\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Recursos y Referencias\n",
        "\n",
        "### Documentación\n",
        "- Grokking Machine Learning, Luis G. Serrano, 2021\n",
        "- [Scikit-learn: SVM](https://scikit-learn.org/stable/modules/svm.html)\n",
        "- [Grid Search Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
        "\n",
        "### Para Aprender Más\n",
        "- Libro: \"Pattern Recognition and Machine Learning\" - Christopher Bishop\n",
        "- Curso: Machine Learning por Andrew Ng (Coursera)\n",
        "\n",
        "---\n",
        "\n",
        "## ¿Preguntas?\n",
        "\n",
        "*Tiempo restante para preguntas y discusión*\n",
        "\n",
        "---\n",
        "\n",
        "## Apéndice: Comparación Rápida con Otros Algoritmos\n",
        "\n",
        "| Algoritmo           | Interpretabilidad | Escalabilidad | Datos No Lineales | Velocidad |\n",
        "|---------------------|-------------------|---------------|-------------------|-----------|\n",
        "| **SVM**             | Media             | Baja          | Excelente         | Media     |\n",
        "| Random Forest       | Alta              | Alta          | Buena             | Alta      |\n",
        "| Redes Neuronales    | Baja              | Media         | Excelente         | Media     |\n",
        "| Regresión Logística | Alta              | Alta          | Limitada          | Alta      |\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
