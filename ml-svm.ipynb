{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "VM3u54NNu4MC",
        "outputId": "8a2a8542-353d-4ae3-8baf-ac84471e6b1e"
      },
      "source": [
        "# MÃ¡quinas de Vectores de Soporte (SVM)\n",
        "## PresentaciÃ³n - Encuentro 4\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Â¿QuÃ© es SVM?\n",
        "\n",
        "### Concepto Simple\n",
        "Las **MÃ¡quinas de Vectores de Soporte (SVM)** son algoritmos que encuentran la **mejor manera de separar** diferentes grupos de datos.\n",
        "\n",
        "### AnalogÃ­a Visual\n",
        "Imagine que tiene **perros** y **gatos** mezclados en un patio. El SVM encuentra la **lÃ­nea mÃ¡s amplia** entre ellos para separarlos claramente.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Los 4 Conceptos Fundamentales\n",
        "\n",
        "### 2.1 Hiperplano\n",
        "- **QuÃ© es**: La lÃ­nea (o plano) que separa las clases\n",
        "- **Objetivo**: Buscar la lÃ­nea que estÃ© lo mÃ¡s lejos posible de ambos grupos\n",
        "- **VisualizaciÃ³n**:\n",
        "  - 2D: una lÃ­nea recta\n",
        "  - 3D: un plano\n",
        "  - nD: un hiperplano (generalizaciÃ³n)\n",
        "\n",
        "    ![](clasificador-c.jpg){width=600px}\n",
        "\n",
        "    [Imagen extraida de Luis G. Serrano - Machine Learning]()\n",
        "\n",
        "### 2.2 Vectores de Soporte\n",
        "- **QuÃ© son**: Los puntos de datos **mÃ¡s cercanos** a la lÃ­nea de separaciÃ³n\n",
        "- **Importancia**: Solo necesitas estos puntos para clasificar nuevos datos\n",
        "- **Ventaja**: Muy eficiente en memoria\n",
        "\n",
        "  **Ejemplo**: 1000 muestras $\\to$ 50 vectores (puntos) de soporte\n",
        "\n",
        "### 2.3 Margen\n",
        "- **QuÃ© es**: El espacio libre entre la lÃ­nea de separaciÃ³n y los puntos mÃ¡s cercanos\n",
        "- **Objetivo**: Maximizar este espacio para crear una separaciÃ³n mÃ¡s robusta\n",
        "- **Tipos**:\n",
        "  - **Margen duro**: No permite errores\n",
        "  - **Margen suave**: Permite algunos errores (mÃ¡s flexible)\n",
        "\n",
        "    ![](clasificador-b.jpg){width=600px}\n",
        "\n",
        "    [Imagen extraida de Luis G. Serrano - Machine Learning]()\n",
        "\n",
        "### 2.4 Kernel\n",
        "- **QuÃ© es**: Una funciÃ³n que permite separaciones **no lineales** (curvas)\n",
        "- **Tipos principales**:\n",
        "  - **Lineal**: Separaciones rectas (mÃ¡s rÃ¡pido)\n",
        "  - **RBF**: Separaciones curvas complejas (mÃ¡s flexible)\n",
        "  - **Polinomial**: Separaciones con curvas polinÃ³micas\n",
        "- **CuÃ¡ndo usar**: RBF para casos generales\n",
        "\n",
        "---\n",
        "\n",
        "## 3. ParÃ¡metros Principales\n",
        "\n",
        "### ParÃ¡metro C (RegularizaciÃ³n)\n",
        "| Valor             | Significado                            | Uso Recomendado               |\n",
        "|-------------------|----------------------------------------|-------------------------------|\n",
        "| C grande (>100)   | LÃ­nea rÃ­gida, pocos errores permitidos | Cuando quieres alta precisiÃ³n |\n",
        "| C pequeÃ±o (<1)    | LÃ­nea flexible, mÃ¡s errores permitidos | Cuando los datos tienen ruido |\n",
        "| **Valor inicial** | C=1                                    | Punto de partida estÃ¡ndar     |\n",
        "\n",
        "### ParÃ¡metro gamma (Solo para kernel RBF)\n",
        "| Valor                | Significado          | Uso Recomendado         |\n",
        "|----------------------|----------------------|-------------------------|\n",
        "| gamma grande (>1)    | Curvas muy complejas | Riesgo de sobreajuste   |\n",
        "| gamma pequeÃ±o (<0.1) | Curvas suaves        | Mejor generalizaciÃ³n    |\n",
        "| **Valor inicial**    | gamma=0.1            | Valor seguro y estÃ¡ndar |\n",
        "\n",
        "### Grid Search\n",
        "- **QuÃ© es**: Prueba automÃ¡ticamente diferentes combinaciones de parÃ¡metros\n",
        "- **Ventaja**: Encuentra la mejor combinaciÃ³n automÃ¡ticamente\n",
        "- **Resultado**: Te entrega el modelo ya optimizado\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Flujo de Trabajo con SVM\n",
        "\n",
        "### Paso a Paso\n",
        "\n",
        "1. **Preparar datos**\n",
        "   - **CRÃTICO**: Normalizar con StandardScaler\n",
        "   - SVM es muy sensible a la escala de los datos\n",
        "\n",
        "2. **Dividir datos**\n",
        "   - 80% para entrenamiento\n",
        "   - 20% para prueba\n",
        "\n",
        "3. **Grid Search**\n",
        "   - Prueba diferentes combinaciones automÃ¡ticamente\n",
        "   - Ya entrena el modelo con los mejores parÃ¡metros\n",
        "\n",
        "4. **Evaluar y visualizar**\n",
        "   - Ver las fronteras de decisiÃ³n\n",
        "   - Calcular mÃ©tricas de rendimiento\n",
        "\n",
        "### Ejemplo: ComparaciÃ³n de SVM con GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SVM (lineal, RBF y polinomial), GridSearchCV, 5-Fold y PCA para visualizaciÃ³n\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 1. Generar datos sintÃ©ticos (puedes cambiar n_features > 2 para probar PCA)\n",
        "X, y = make_blobs(n_samples=400, centers=2, n_features=5, cluster_std=4, random_state=42)\n",
        "\n",
        "# 2. Dividir en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# 3. Definir la grilla de hiperparÃ¡metros\n",
        "param_grid = [\n",
        "    {'kernel': ['linear'], 'C': [0.1, 1, 10]},\n",
        "    {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': ['scale', 0.1, 1]},\n",
        "    {'kernel': ['poly'], 'C': [0.1, 1, 10], 'degree': [2, 3, 4], 'gamma': ['scale', 0.1, 1]}\n",
        "]\n",
        "\n",
        "# 4. Crear el modelo base y el GridSearchCV con validaciÃ³n cruzada (5 folds)\n",
        "svm = SVC()\n",
        "grid = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 5. Mostrar los mejores parÃ¡metros\n",
        "print(\"ğŸ§  Mejor combinaciÃ³n encontrada por GridSearchCV:\")\n",
        "print(grid.best_params_)\n",
        "print(f\"Mejor puntuaciÃ³n media de validaciÃ³n: {grid.best_score_:.3f}\")\n",
        "\n",
        "# 6. Evaluar el mejor modelo\n",
        "best_svm = grid.best_estimator_\n",
        "y_pred = best_svm.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nğŸ¯ PrecisiÃ³n en el conjunto de prueba: {acc:.3f}\")\n",
        "print(\"\\nğŸ“Š Reporte de clasificaciÃ³n:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 7. Matriz de confusiÃ³n\n",
        "ConfusionMatrixDisplay.from_estimator(best_svm, X_test, y_test, cmap='Blues')\n",
        "plt.title(\"Matriz de confusiÃ³n del mejor SVM\")\n",
        "plt.show()\n",
        "\n",
        "# 8. ReducciÃ³n a 2D con PCA (solo para visualizaciÃ³n)\n",
        "if X.shape[1] > 2:\n",
        "    print(f\"\\nğŸ“‰ Aplicando PCA: reducciÃ³n de {X.shape[1]} â†’ 2 dimensiones para visualizaciÃ³n.\")\n",
        "    pca = PCA(n_components=2)\n",
        "    X_vis = pca.fit_transform(X)\n",
        "else:\n",
        "    X_vis = X  # si ya tiene 2 dimensiones, no aplicamos PCA\n",
        "\n",
        "# 9. VisualizaciÃ³n de fronteras de decisiÃ³n\n",
        "# âš ï¸ Usamos PCA solo para graficar, no para entrenar el modelo (por claridad)\n",
        "# En escenarios reales, puedes aplicar PCA antes del entrenamiento si lo deseas.\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "# Crear una malla 2D\n",
        "x_min, x_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1\n",
        "y_min, y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
        "                     np.linspace(y_min, y_max, 300))\n",
        "\n",
        "# Si los datos fueron reducidos con PCA, proyectamos la malla al espacio original para predecir\n",
        "if X.shape[1] > 2:\n",
        "    # Invertimos la proyecciÃ³n para estimar el espacio original\n",
        "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    grid_points_original = pca.inverse_transform(grid_points)\n",
        "    Z = best_svm.predict(grid_points_original)\n",
        "else:\n",
        "    Z = best_svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Dibujar fronteras\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='Accent')\n",
        "plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y, cmap='Accent', edgecolors='k')\n",
        "plt.title(f\"Fronteras de decisiÃ³n del mejor SVM ({grid.best_params_['kernel']})\")\n",
        "plt.xlabel(\"Componente 1 (PCA)\" if X.shape[1] > 2 else \"CaracterÃ­stica 1\")\n",
        "plt.ylabel(\"Componente 2 (PCA)\" if X.shape[1] > 2 else \"CaracterÃ­stica 2\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Â¿CuÃ¡ndo Usar SVM?\n",
        "\n",
        "### âœ… Ventajas\n",
        "- **Eficiente en memoria**: Solo almacena vectores de soporte\n",
        "- **Bueno con alta dimensionalidad**: Funciona bien con muchas caracterÃ­sticas\n",
        "- **Flexible**: Diferentes kernels para diferentes tipos de separaciÃ³n\n",
        "- **Robusto**: Resiste datos atÃ­picos\n",
        "\n",
        "### âš ï¸ Desventajas\n",
        "- **Lento con datasets muy grandes** (>10,000 muestras)\n",
        "- **Requiere normalizaciÃ³n** (no opcional)\n",
        "- **ParÃ¡metros sensibles** (requieren ajuste)\n",
        "\n",
        "### ğŸ¯ CuÃ¡ndo Usar\n",
        "- Dataset pequeÃ±o a mediano (<10,000 muestras)\n",
        "- Necesitas separaciones no lineales\n",
        "- Trabajas con alta dimensionalidad\n",
        "- Necesitas un modelo robusto\n",
        "\n",
        "---\n",
        "\n",
        "## 6. MÃ©tricas de EvaluaciÃ³n\n",
        "\n",
        "### Matriz de ConfusiÃ³n\n",
        "\n",
        "| Real \\ PredicciÃ³n | Positiva        | Negativa        |\n",
        "|-------------------|-----------------|-----------------|\n",
        "| **Positiva**      | **VP** o **TP** | FN              |\n",
        "| **Negativa**      | FP              | **VN** o **TN** |\n",
        "\n",
        "#### Ejemplo:\n",
        "\n",
        "| Real \\ Predicho | Perro       | Gato        | PÃ¡jaro      |\n",
        "|-----------------|-------------|-------------|-------------|\n",
        "| **Perro**       | **70 (TP)** | 5 (FN)      | 2 (FN)      |\n",
        "| **Gato**        | 10 (FP)     | **85 (TP)** | 5           |\n",
        "| **PÃ¡jaro**      | 0 (FP)      | 3           | **95 (TP)** |\n",
        "\n",
        "\n",
        "### MÃ©tricas Principales\n",
        "\n",
        "**FÃ³rmulas bÃ¡sicas:**\n",
        "- **Accuracy**: (TP + TN) / Total\n",
        "- **Precision**: TP / (TP + FP)  \n",
        "- **Recall**: TP / (TP + FN)\n",
        "- **F1-Score**: 2 Ã— (Precision Ã— Recall) / (Precision + Recall)\n",
        "\n",
        "| MÃ©trica       | InterpretaciÃ³n                                |\n",
        "|---------------|-----------------------------------------------|\n",
        "| **Accuracy**  | Porcentaje de clasificaciones correctas      |\n",
        "| **Precision** | De los predichos positivos, Â¿cuÃ¡ntos son realmente positivos? |\n",
        "| **Recall**    | De los realmente positivos, Â¿cuÃ¡ntos detectamos? |\n",
        "| **F1-Score**  | Balancea precisiÃ³n y recall (media armÃ³nica)  |\n",
        "\n",
        "### Â¿QuÃ© MÃ©trica Usar?\n",
        "\n",
        "| Escenario                    | MÃ©trica Principal | RazÃ³n                           |\n",
        "|------------------------------|-------------------|---------------------------------|\n",
        "| Clases balanceadas           | **Accuracy**      | Representa bien el rendimiento  |\n",
        "| Clase minoritaria importante | **F1-Score**      | Balancea ambas mÃ©tricas         |\n",
        "| Evitar falsas alarmas        | **Precision**     | Ej: spam vs no-spam             |\n",
        "| Detectar todos los casos     | **Recall**        | Ej: detecciÃ³n de fallas, cÃ¡ncer |\n",
        "| Clases desbalanceadas        | **Macro F1**      | No se sesga por mayorÃ­a         |\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Mejores PrÃ¡cticas\n",
        "\n",
        "### âœ… Hacer SIEMPRE\n",
        "1. **Normalizar datos** (crÃ­tico)\n",
        "2. Usar **Grid Search** para encontrar mejores parÃ¡metros\n",
        "3. **ValidaciÃ³n cruzada** para evaluar robustez\n",
        "4. Comparar **train vs test accuracy** para detectar sobreajuste\n",
        "5. Empezar con valores conservadores de C y gamma\n",
        "\n",
        "### âŒ Errores Comunes\n",
        "1. **No normalizar** datos de prueba correctamente\n",
        "2. **Usar datos de prueba durante entrenamiento**\n",
        "3. **Ignorar sobreajuste** (train accuracy alta, test baja)\n",
        "4. **Valores extremos** de C y gamma sin validar\n",
        "5. **No visualizar** las fronteras de decisiÃ³n\n",
        "\n",
        "### ğŸ¯ InterpretaciÃ³n de Resultados\n",
        "- **Pocos vectores de soporte**: Modelo simple y generalizable âœ…\n",
        "- **Muchos vectores de soporte**: Modelo complejo (posible sobreajuste) âš ï¸\n",
        "- **Train accuracy alta, test baja**: Sobreajuste (reducir C o gamma)\n",
        "- **Baja accuracy en ambos**: Bajo rendimiento (cambiar kernel o parÃ¡metros)\n",
        "\n",
        "---\n",
        "\n",
        "## 8. El Camino de los Datos: Entrenamiento â†’ PredicciÃ³n\n",
        "\n",
        "  ![](ml-svm-blocos_diagram_01.jpg)\n",
        "---\n",
        "\n",
        "## 9. Entrenamiento del modelo\n",
        "\n",
        "  ![](ml-svm-blocos_diagram_02.jpg){width=1200px}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Puntos Clave para Recordar\n",
        "\n",
        "### ğŸ¯ Los 3 Conceptos Esenciales\n",
        "1. **Hiperplano**: La lÃ­nea que separa clases\n",
        "2. **Vectores de Soporte**: Puntos clave que definen la separaciÃ³n\n",
        "3. **Margen**: Espacio que queremos maximizar\n",
        "\n",
        "### ğŸ”§ Los 2 ParÃ¡metros CrÃ­ticos\n",
        "1. **C**: Controla cuÃ¡nto errores permitir (C grande = menos errores)\n",
        "2. **gamma**: Controla complejidad de curvas (gamma pequeÃ±o = mÃ¡s simple)\n",
        "\n",
        "### âœ… El Flujo Clave\n",
        "1. Normalizar âš ï¸ (SIEMPRE)\n",
        "2. Grid Search (optimizaciÃ³n automÃ¡tica)\n",
        "3. Visualizar (entender el modelo)\n",
        "4. Evaluar (mÃ©tricas adecuadas)\n",
        "\n",
        "---\n",
        "\n",
        "## 11. PrÃ³ximos Pasos\n",
        "\n",
        "### Para Practicar\n",
        "1. âœ… Ejecutar el notebook de prÃ¡ctica\n",
        "2. âœ… Probar diferentes kernels (lineal, RBF)\n",
        "3. âœ… Cambiar parÃ¡metros C y gamma\n",
        "4. âœ… Aplicar a dataset real (ej: Iris)\n",
        "5. âœ… Experimentar con datos multiclase\n",
        "\n",
        "### Para Profundizar\n",
        "- Explorar SVM multiclase\n",
        "- Implementar en casos industriales reales\n",
        "- Comparar con otros algoritmos (Random Forest, Redes Neuronales)\n",
        "- Estudiar teorÃ­a matemÃ¡tica de SVM\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Recursos y Referencias\n",
        "\n",
        "### DocumentaciÃ³n\n",
        "- Grokking Machine Learning, Luis G. Serrano, 2021\n",
        "- [Scikit-learn: SVM](https://scikit-learn.org/stable/modules/svm.html)\n",
        "- [Grid Search Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
        "\n",
        "### Para Aprender MÃ¡s\n",
        "- Libro: \"Pattern Recognition and Machine Learning\" - Christopher Bishop\n",
        "- Curso: Machine Learning por Andrew Ng (Coursera)\n",
        "\n",
        "---\n",
        "\n",
        "## Â¿Preguntas?\n",
        "\n",
        "*Tiempo restante para preguntas y discusiÃ³n*\n",
        "\n",
        "---\n",
        "\n",
        "## ApÃ©ndice: ComparaciÃ³n RÃ¡pida con Otros Algoritmos\n",
        "\n",
        "| Algoritmo           | Interpretabilidad | Escalabilidad | Datos No Lineales | Velocidad |\n",
        "|---------------------|-------------------|---------------|-------------------|-----------|\n",
        "| **SVM**             | Media             | Baja          | Excelente         | Media     |\n",
        "| Random Forest       | Alta              | Alta          | Buena             | Alta      |\n",
        "| Redes Neuronales    | Baja              | Media         | Excelente         | Media     |\n",
        "| RegresiÃ³n LogÃ­stica | Alta              | Alta          | Limitada          | Alta      |\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
